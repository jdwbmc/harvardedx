VIJAY JANAPA REDDI: Given that you've now completed course 1, 2, and 2,
you might be asking yourself this question of what do I do now.
Well first and foremost, I want you to stop and reflect on the fact
that you've come a very long way.
Recognize that in going from fundamentals of TinyML,
to learning about the applications of TinyML, to actually deploying TinyML,
you went through this complete machine learning workflow,
going from collecting data to making inferences.
What I want to do now is to kind of step you through each one of those cells,
in order to help you kind of reflect back on the critical items.
And then I'll use that as a seed to kind of give you
new ideas about things that you might be able to do.
Recall that when it comes to dealing with data,
it's extremely important to know how you're going to collect the right data.
In this way, we actually thought you how to do your own custom data
collection in course 3.
Hopefully, that exercise taught you about all the things
that you have to think about in terms of carefully identifying
the requirements for your data set.
Now it's one thing to hear about the requirements, which we explained to you
through speech commands, for example.
But it's a whole different thing when you actually
have to go about in collecting data.
So hopefully, that exercise kind of empowers you
into understanding the value of data sets
and how you might be able to go ahead in building new data sets.
Second, we talked a lot about pre-processing data.
Now we talked about pre-processing data in the form of data augmentation
in course 1.
But especially in course 2 and course 3, we
emphasized the role of pre-processing, where we take signals in one domain
and might convert them into a totally different representation.
In other words, we took an audio signal and converted that
into a spectrogram image, and then used that to be
able to actually detect what the audio word was,
which is a remarkable sort of process that we did.
And that's only possible because we pre-process the data very intelligent.
And that's quite crucial for any ML application.
But especially in the context of TinyML, I
want you to think about what it means to pre-process sensor data that's
coming in.
Because we don't think about this carefully,
you might accidentally make the mistake of getting
a rich amount of data that would not really
work well through a neural network.
For example, we talked about end-to-end learning
in the context of taking IMU sensor data and feeding it directly
into the neural network, instead of going through the pre-processing stage
where we use traditional methods to rasterize the image into a 2D plane.
That's an example.
Next, we talked about designing a model.
Well, mostly I hope you understood that when
you're thinking of deployment scenarios, you really
have to think very carefully about where you're deploying the model.
So because we have very limited amount of microcontroller memory,
we thought very carefully about making sure
that we had a small neural network that would easily fit on both the SRM,
as well as the flash.
So we need to be conscious about actually designing the models.
And so, as you go on to build new TinyML models, keep this in your mind.
And there's also other stuff you might be
able to pick up like neural architecture search, which helps you automatically
try and find the right small little neural network that would perform best
from an accuracy sorted standpoint.
So there's much more out there.
But hopefully, you realize that the key nugget here
is that you've got to have a small model that achieves reasonable accuracy.
Then, of course, you learned about how to train models, especially
in course 1.
And the key takeaway here is really understanding
how we get the original data set, which again, goes back
to the notion of collecting data.
And then you learn how that data can actually
be split into training set and the test set.
And the training set can further be broken into really the training
set that's actually used for evaluating the model, doing the hyperparameter
tuning space, and then, how the test is ultimately used to validate
the accuracy of the final model.
And you also get exposed to doing training using Google Colab,
rather than using your own resources.
Then of course, we talked a lot about evaluating and optimizing models.
Specifically, in course 1 for instance, we
talked about underfitting and overfitting.
You don't want the network to underfit.
And you don't want the network to overfit.
You got to try and find the right amount of balance
in order to get the best experience from your neural network.
And in that process as we went into course 2,
we looked at how the accuracy of the model
changes when you start throwing in new data,
and how it works across different kinds of accents and so forth.
And we also learned about optimizing the models.
Remember, you typically end up with a Float-32 model.
But Float-32 models and not good for microcontrollers,
which sometimes might not, in fact, have floating point support.
And you also learned that floating point is much more expensive
in terms of computation, as compared to integer operations.
And so one of the things you want to do, is
you want to quantize the models, both for size, as well as latency.
And in that process, you learned about PTQ and QAT,
where PTQ stands for post training quantization,
and QAT stands for quantization and weight training.
In other words, after you quantize the model,
you retrain the model in order to get back the accuracy loss that
happens as a result of quantization.
Now that's a very powerful concept.
By and large, if you look at TinyML systems,
you're almost always going to have to have a quantized model.
Right now, we're talking about int8 quantized model.
But there are architectures out there that are starting
to support int4 models, for instance.
So keep this in mind, because you're definitely
going to want to reuse quantization with whatever application
you do, because all microcontrollers at the end of the day,
are going to run much more efficiently with int8.
And then you learned the process of actually converting
the model into a flat buffer format.
So it's actually widely deployable across a wide range of devices.
Specifically at this point, you learn the differences
between the different types of frameworks
you have for machine learning, going all the way from TensorFlow to TensorFlow
Lite, to then TensorFlow Lite Micro.
Hopefully, if one was to ask you, hey, what is the difference
between the three of them?
You're able to explain this from the perspective of whether you
need to train or not, how many different operators you need to support,
what kind of operating system support is needed to run these systems,
and so forth.
And so there's a rich amount of knowledge you picked up on this.
And so when you go on to build your own application, reflect on this
and see what is a right sort of tool that you need to have.
While we talked a lot about TensorFlow Lite and Micro
in this particular course, I did try and give you hints
about how other systems also work.
So keep an open mind.
And go and explore other frameworks is the real key takeaway here.
And of course, when it came down to actually deploying things,
we did a lot more on TensorFlow Lite Micro, which is a production grade
inference framework for TinyML.
So go ahead and use the APIs that you learned, because you can actually
build production grade systems using the exact code that we have actually
shown you.
Now, you would also have picked up the fact
that if you wanted to build systems like TensorFlow Lite Micro,
you would have an appreciation for understanding the complexities.
Why do we need to build a system like TensorFlow Lite Micro?
Well, that's because there is lots of problems
when it comes to the embedded ecosystem, which is really fragmented.
So reflect back on some of those readings we gave you,
which explain the hardware and software differences--
the heterogeneity, the resource constraints, the missing operating
system sort of support all of the things that we typically
take for granted in much larger systems.
The lack of that sort of support makes it extremely crucial
for us to learn how to build these lean and mean inference engines.
Finally, you learned the art of making inferences through the experiences
that you've had with Keyword Spotting, Visual Wake Words, and the Magic Wand.
Through these application deployments, I hope
you realize some of the pitfalls of these application.
They don't always work well.
But that's not the point of this course.
It's not about getting things working well.
It's about getting things working.
Because the lessons here, really are about when you get the things working,
then you start experiencing when something works
and when something does not work.
That is what's going to make you a strong tangible engineer.
Because you have an appreciation for where the issues are really
going to stem from.
All that said-- now that you've got this vast amount of knowledge going
all the way from collecting data to making inferences,
the question in your head might be, hey, what some new application I can unlock?
Well that's a great question to have.
I just want you to keep a certain mentality in mind.
I want you to focus on the Golden Circle approach
when you're trying to build your next TinyML application.
It does not matter how big or small your application is,
but always focus first, on the question of why.
Why do you want to build this application?
What is it that is actually interesting?
What's the purpose?
Why is this particular TinyML application
going to unlock the future in a way that you see it?
Then, focus on how.
What is your value proposition?
What is the unique approach that you bring to the table that separates you
from someone else who might be interested in building a same TinyML
sort of solution?
And finally, then think about what, how you actually went about it.
So don't focus on how.
Just because all the different things you have thought you,
don't start there.
Instead, I'm asking you to flip your mindset,
and go into identifying the root cause of wanting to build an application.
And then work your way outwards because if you
do that, you will end up building a successful TinyML application.
And that's exactly what I want to see.
I personally, can guarantee you that the success from this course for me,
comes not so much from how many students are taking it,
but much more from what are the things that you're
building now that you have learned all these things that we have taught you.
With that said, remember that the future of ML is tiny and bright.
And I'm looking forward to all the amazing things
that come out of all the exciting projects are you going to be building.
Have fun.