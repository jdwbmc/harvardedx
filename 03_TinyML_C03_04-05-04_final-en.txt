PROFESSOR: This is the first of many screencasts
that I will be doing in order to help you understand some of the TensorFlow
Lite Micro code in a bit more detail than we looked at in the videos
previously.
As you may recall, there are many different components
involved in the Keyword Spotting application,
starting with the initialization phase.
In this particular screencast, I'm going to focus on the initialization stage.
And I'm going to walk you through the different steps that are involved.
I'm going to do this in a lot of detail this particular time around,
but in subsequent applications, I will go a little bit
faster just because it is a repetitive template that you'll find
used in many different applications.
So looking at our IDE, the first thing you will see here
is that there are a couple of header files.
Many of these header files are all about the TensorFlow Lite Micro.
For example, line 26 through 31 are all about setting up the default TensorFlow
Lite Micro ops.
There are a couple of application-specific files here.
Now, these correspond to activities that are
part of the application architecture, which we'll get into later.
Now, going back to my, slides one of the first steps
is to actually declare the variables.
So let's look at what the code says.
Scrolling down a little bit, between line 34 and line 50, what you see
are the actual variable declarations that are needed.
The first part of the code is mostly centered around things
that are needed for the TensorFlow Lite Micro interpreter itself.
There is an ErrorReporter which is responsible for logging errors.
You will find this in almost every single application.
Then there is the model pointer, which actually holds the flat buffer
model that we need to load that corresponds to application.
For example, here we're interested in the tinycom
network that does the keyword spotting.
Then we have the interpreter itself, which
we have to initialize with a certain amount of required state.
Then we have an input pointer for the input
that's going into the neural network.
And the next two correspond to actually the application specifics,
which are about getting the features that are needed, in other words,
where is the spectrogram sitting, and the output that comes out
of the network, which is stored in recognized commands, which tells us
if a keyword was actually recognized or not--
more on the previous time later.
And the second half of the variable declaration scope
says that we first have to define the tensorArena.
Now, if you remember from what Pete said and what I have said before,
the tensorArena is extremely critical.
And you have to size it appropriately if you do not want your application
to crash with an error.
So here you have experimentally determined what the size needs to be.
And then you go ahead and allocate the tensorArena buffer,
which has got a certain size.
Then we allocate the feature buffer, which
contains the actual spectrogram features which we will be using in order
to do the inference on.
And it's of a certain size as well.
And then we have the model input buffer.
This is basically pointing to a buffer that
will be initialized later in order to tell the interpreter where
the inputs are coming from.
The next step in our timeline is to actually load the model.
So if I was to scroll down a little bit and go into the setup routine, which,
as you should recall, is only executed once,
you will find that here there's a function called GetModel.
GetModel's job is effectively to take a pointer to a single-dimensional array
which has actually got the flat buffer file
schema, which defines the neural network model that we want to load and run.
And the next couple of lines of basically doing some error checking
to make sure that the schema version matches, because you always
want to make sure that the network output file that you have
in your schema is actually corresponding to the latest TensorFlow version
that we will be using.
So this is quite standard.
And I expect you to do the same.
After that comes resolving operators.
Scrolling down a little bit further, you will
see that between line 79 and line 99, we have a chunk of code which
is all about the op resolver.
The op resovler is taking in four different ops in here.
Now, how do you know what the ops are?
Well, you should have
looked up what the ops are using the Netrun
app that I told you about earlier.
And if you look at the network architecture using Netrun,
you would know that these are the different ops.
Now, if you do not know what ops that network is actually using,
let's say, hypothetically, you missed one of the ops,
it's not the end of the world.
TF Lite Micro will actually tell you through an error
that you are missing one of the ops when it's actually trying to run it.
So that's one of the ways you can easily figure out,
what are all the options that you want to include in your ops resolver?
As Pete and I both mentioned, you have the AllOps resolver,
which is a cop-out way of doing op registration.
In other words, if you do not want to explicitly register all these ops,
you can easily say just use all ops, bringing all the ops.
But of course, as a TinyML engineer, at this point
you should know what the pros and cons are of bringing in all the ops.
Bringing in all the DevOps is not necessarily a good thing,
because it ends up wasting a lot of space.
So keep that in mind.
Then as we scroll down further, you will see
that we are arriving at the next major step, which
is initializing the interpreter.
Initializing the interpreter requires us to pass a whole bunch of variables
in here, starting with the model that we're actually bringing in,
what are the ops, where is the tensorArena, how big is it,
and what is the way by which we have to report errors.
Once you have this bit of code here, you are effectively
ready to start using the interpreter.
Then going back to our timeline, we see that the next step
is to allocate the arena, which is what is happening on line 107.
We are actually allocating the tensors.
This is using the tensorArena space that we had previously
told the interpreter to use.
Once again, we do error checking, as always.
It's a good practice to have.
Finally, after that, we tell the neural network
where the model's inputs are really coming from.
And that is happening between line 113 and line 123.
So what we are asking the interpreter to do
is to give us a reference to where its input buffers are actually stored.
Now, you will see that it says the model input has got a dimension of 2.
Now, you might be wondering, why does the input buffer have two dimensions?
Well the first one is purely a wrapper.
That's why you're seeing that here, when we
look at the dimensions of the data of the first element,
it's got only one entry in it.
And that single entry, which you, then, if you poke at,
will just have a single size one.
And subsequently, the second entry that's in there
is actually our spectrogram size.
In other words, it is a k feature slice count times k feature slice.
Now, if you remember how big our spectrogram is,
it's going to be 40 columns wide.
And it's going to be 49 rows in height.
And that's what this is, slice count times slice size.
So if you want to look this up, let's do a quick search.
I'm going to do k feature slice size.
I'm going to find it.
It's used in a couple of places.
But we'll get to the actual definition soon.
There it is.
It's declared as a constant.
And you can see the k slice size is 40, which is effectively our column size.
And k slice count is 49, which is the number
of rows we have in our spectrograms.
And so now if we go back up to our micro speech file,
we're just making sure that the spectrogram dimensions which
we are going to be feeding into the network
are exactly the same as what the network is expecting.
So this is actually what the network is expecting.
And of course, we've got to make sure this is the size that we pass it in.
And we want to make sure that the data type is correct, that it's not a fp32,
it's actually an int8 value that we're passing in.
And that's effectively what we're defining here.
Finally, the next step in our process is to set up the main loop.
In setting up the main loop, you can clearly
see down here is that there are two stages here.
The first one is called setting up the feature provider.
The feature provider is the one that actually accesses the audio buffer
and generates spectrogram.
And that spectrogram is what we actually feed into the network later on.
And after you do the inference, that's when
we go into recognize commands, which will actually help us recognize.
More on this later-- for now, all you need to know
is that we're just simply setting these variables up.
All right, with all of that said, you should at this point
be quite comfortable having gone all the way from declaring variables,
to loading the model, to resolving the ops, to initializing the interpreter,
to allocating the arena, to defining the model inputs
and setting up the main loop.
This is a process that we will repeat over, and over, and over
in all the different applications.
So get comfortable with it.
And you will naturally be feeling comfortable with it
as you start writing a little bit more code.
With that said, I'll pick you up in the next video.